{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8574749",
   "metadata": {},
   "source": [
    "# Prelim experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07228f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d2855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loadDataset(datasetPath = '../ADL_DCASE_DATA/'):\n",
    "    \n",
    "    def loadAudio(path):\n",
    "        files = listdir(path)\n",
    "        audioDataset = []\n",
    "        for file in files:\n",
    "            audioDataset.append((file, np.load(path + \"/\" + file)))\n",
    "        return audioDataset\n",
    "    \n",
    "    def loadCSV(path):\n",
    "        return pd.read_csv(path, names = [\"file\",\"category\"], index_col = \"file\")\n",
    "    \n",
    "    def getFileCategory(InputFeat, OutputFeat):\n",
    "        return [(OutputFeat.loc[file][0], audio) for (file, audio) in InputFeat]\n",
    "    \n",
    "#     trainingInput = loadAudio(datasetPath + \"development/audio\")\n",
    "    \n",
    "    trainingGroundTruth = loadCSV(datasetPath + \"development/labels.csv\")\n",
    "        \n",
    "#     trainingDataset = getFileCategory(trainingInput, trainingGroundTruth)\n",
    "\n",
    "    \n",
    "    return trainingGroundTruth[\"category\"].tolist()\n",
    "    \n",
    "    \n",
    "categories = loadDataset()\n",
    "ind_categories = []\n",
    "for category in categories:\n",
    "    if category not in ind_categories:\n",
    "        ind_categories.append(category)\n",
    "        \n",
    "\n",
    "print(ind_categories)\n",
    "print(len(ind_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c344c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AudioEnviromentDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.fileCategory = pd.read_csv(self.root_dir + \"/labels.csv\", names = [\"file\",\"category\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fileCategory)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.fileCategory.iloc[idx][\"file\"]\n",
    "        category = ind_categories.index(self.fileCategory.iloc[idx][\"category\"])\n",
    "        return np.load(self.root_dir + \"/audio/\" + file).reshape(-1, 60, 1501), category\n",
    "    \n",
    "training_data = AudioEnviromentDataset(\"../ADL_DCASE_DATA/development\")\n",
    "test_dataloader = AudioEnviromentDataset(\"../ADL_DCASE_DATA/evaluation\")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = 32)\n",
    "test_dataloader = DataLoader(training_data, batch_size = 32)\n",
    "\n",
    "print(training_data[0][0].shape)\n",
    "print(training_data[0][1])\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(training_data[0][0][0], aspect=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54028d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 60, 1501\n",
    "# y = 60\n",
    "# x = 1501\n",
    "\n",
    "\n",
    "# y = y\n",
    "# x = x\n",
    "\n",
    "# x = x / 2\n",
    "# y = y / 2\n",
    "\n",
    "# x = x / 2\n",
    "# y = y / 2\n",
    "\n",
    "# print(64 * x * y)\n",
    "# print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4609e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_neuro_stack = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=(5, 5),\n",
    "                padding=(2, 2),\n",
    "            ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=2,\n",
    "                kernel_size=(5, 5),\n",
    "                padding=(2, 2),\n",
    "            ),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            \n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.BatchNorm1d(11250),\n",
    "            nn.Linear(11250,1000),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Linear(1000,15)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.cnn_neuro_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "model = AudioCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ab417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# a = torch.randn(3, 5, requires_grad=True)\n",
    "# b = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "# loss(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a332a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 5 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")        \n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45297b8f",
   "metadata": {},
   "source": [
    "# Writing training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee04f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dcase as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c17da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = dc.DCASE(\"../ADL_DCASE_DATA/development/\" , 15)\n",
    "print(\"datapoints:\", len(training_data))\n",
    "print(\"segments per point:\", len(training_data[0][0]))\n",
    "plt.imshow(training_data[0][0][0], aspect=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e728c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = dc.DCASE(\"../ADL_DCASE_DATA/development/\" , 30)\n",
    "plt.figure(figsize=(15,15))\n",
    "print(\"datapoints:\", len(training_data))\n",
    "print(\"segments per point:\", len(training_data[0][0]))\n",
    "plt.imshow(training_data[0][0][0], aspect=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c61d8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = dc.DCASE(\"../ADL_DCASE_DATA/development/\" , 3)\n",
    "print(\"segments per point:\", len(training_data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f408da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data[0][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(6).reshape(2,3)\n",
    "midpoint = (a.max() + a.min())/2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.matshow(a, cmap='Greens')\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_yticks(range(2))\n",
    "ax.set_xticklabels([\"pee\",\"poo\",\"fart\"])\n",
    "ax.set_xlabel(\"What happened\")\n",
    "ax.set_yticklabels([\"pee\",\"poo\"])\n",
    "ax.set_ylabel(\"What happens next\")\n",
    "\n",
    "for (i, j), z in np.ndenumerate(a):\n",
    "    if z > midpoint:\n",
    "        ax.text(j, i, '{:1}'.format(z), color='white')\n",
    "    else:\n",
    "        ax.text(j, i, '{:1}'.format(z))\n",
    "\n",
    "# fig.canvas.draw()\n",
    "# img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "# img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "# img = img / 255.0\n",
    "\n",
    "# print(fig.canvas.tostring_rgb())\n",
    "\n",
    "canvas = FigureCanvas(fig)\n",
    "canvas.draw()\n",
    "\n",
    "image = np.array(canvas.renderer.buffer_rgba())\n",
    "image = image/255\n",
    "fig, ax = plt.subplots()\n",
    "print(image.shape)\n",
    "plt.imshow(image)\n",
    "image_y, image_x = image.shape[0], image.shape[1]\n",
    "image = image[:,:,0:3].reshape(3, image_y, image_x)\n",
    "print(image.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
